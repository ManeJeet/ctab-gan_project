{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model\n",
    "from model.ctabgan import CTABGAN\n",
    "# Importing the evaluation metrics \n",
    "from model.eval.evalFidel import evaluate_fidelity\n",
    "# Importing standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the replication number \n",
    "num_exp = 1 \n",
    "# Specifying the name of the dataset used \n",
    "dataset = \"car\" \n",
    "# Specifying the path of the dataset used \n",
    "real_path = \"Real_Datasets/Datasets/Car/car.csv\" \n",
    "# Specifying the root directory for storing generated data\n",
    "fake_file_root = \"Fake_Datasets\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Fitting the synthesizer to the training dataset and generating synthetic data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_exp):\n\u001b[0;32m---> 15\u001b[0m     synthesizer\u001b[39m.\u001b[39mfit()\n\u001b[1;32m     16\u001b[0m     syn \u001b[39m=\u001b[39m synthesizer\u001b[39m.\u001b[39mgenerate_samples()\u001b[39m.\u001b[39miloc[:\u001b[39m1000\u001b[39m]\n\u001b[1;32m     17\u001b[0m     syn\u001b[39m.\u001b[39mto_csv(fake_file_root\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mdataset\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m dataset\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_fake_\u001b[39m\u001b[39m{exp}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(exp\u001b[39m=\u001b[39mi), index\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/project/CTAB-GAN/CTAB-GAN-NEW/model/ctabgan.py:54\u001b[0m, in \u001b[0;36mCTABGAN.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     53\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_prep \u001b[39m=\u001b[39m DataPrep(\n\u001b[1;32m     55\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_df,\n\u001b[1;32m     56\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_columns,\n\u001b[1;32m     57\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_columns,\n\u001b[1;32m     58\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_columns,\n\u001b[1;32m     59\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minteger_columns,\n\u001b[1;32m     60\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type,\n\u001b[1;32m     61\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_ratio\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msynthesizer\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     64\u001b[0m         train_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_prep\u001b[39m.\u001b[39mdf,\n\u001b[1;32m     65\u001b[0m         categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_prep\u001b[39m.\u001b[39mcolumn_types[\u001b[39m\"\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m     \u001b[39m# The following line appears to be unused. If it is not needed, consider removing it.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[39m# mixed = self.data_prep.column_types[\"mixed\"], type=self.problem_type\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/project/CTAB-GAN/CTAB-GAN-NEW/model/pipeline/data_preparation.py:45\u001b[0m, in \u001b[0;36mDataPrep.__init__\u001b[0;34m(self, raw_df, categorical, log, mixed, integer, type, test_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m y_real \u001b[39m=\u001b[39m raw_df[target_col]\n\u001b[1;32m     44\u001b[0m X_real \u001b[39m=\u001b[39m raw_df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[target_col])\n\u001b[0;32m---> 45\u001b[0m X_train_real, _, y_train_real, _ \u001b[39m=\u001b[39m model_selection\u001b[39m.\u001b[39mtrain_test_split(X_real ,y_real, test_size\u001b[39m=\u001b[39mtest_ratio, stratify\u001b[39m=\u001b[39my_real,random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)        \n\u001b[1;32m     46\u001b[0m X_train_real[target_col]\u001b[39m=\u001b[39m y_train_real\n\u001b[1;32m     48\u001b[0m \u001b[39m# Replacing empty strings with na if any and replace na with empty\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2801\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2797\u001b[0m         CVClass \u001b[39m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2799\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2801\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m   2803\u001b[0m train, test \u001b[39m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[39m0\u001b[39m], train, test)\n\u001b[1;32m   2805\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2806\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2807\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2808\u001b[0m     )\n\u001b[1;32m   2809\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2334\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[39mif\u001b[39;00m groups \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2330\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2331\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe groups parameter is ignored by \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2332\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   2333\u001b[0m     )\n\u001b[0;32m-> 2334\u001b[0m y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   2335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[39m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[39m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[39m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# Initializing the synthesizer object and specifying input parameters\n",
    "# Notice: If you have continuous variable, you do not need to explicitly assign it. It will be treated like \n",
    "# that by default\n",
    "synthesizer =  CTABGAN(raw_csv_path = real_path,\n",
    "                 test_ratio = 0.20,  \n",
    "                 categorical_columns = ['Buying','Maint','Doors','Persons','Lug_boot','Safety','Class'], \n",
    "                 log_columns = [],\n",
    "                 mixed_columns= {}, \n",
    "                 integer_columns = [],\n",
    "                 problem_type= {\"Classification\":'Class'},\n",
    "                 epochs = 300) \n",
    "\n",
    "# Fitting the synthesizer to the training dataset and generating synthetic data\n",
    "for i in range(num_exp):\n",
    "    synthesizer.fit()\n",
    "    syn = synthesizer.generate_samples().iloc[:1000]\n",
    "    syn.to_csv(fake_file_root+\"/\"+dataset+\"/\"+ dataset+\"_fake_{exp}.csv\".format(exp=i), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the paths to all corresponding generated datasets for evaluation \n",
    "fake_paths = glob.glob(fake_file_root+\"/\"+dataset+\"/\"+\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_paths=\"Fake_Datasets/\"+dataset+\"/\"+dataset+\"_fake_0.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test 80 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression (trained on synthetic, tested on real 20%)\n",
      "  Accuracy : 0.6791907514450867\n",
      "  F1 Score : 0.5494314168316536\n",
      "  Precision: 0.4613000768485415\n",
      "  Recall   : 0.6791907514450867\n",
      "\n",
      "Random Forest (trained on synthetic, tested on real 20%)\n",
      "  Accuracy : 0.49710982658959535\n",
      "  F1 Score : 0.492355100233369\n",
      "  Precision: 0.4924972961265665\n",
      "  Recall   : 0.49710982658959535\n",
      "\n",
      "XGBoost (trained on synthetic, tested on real 20%)\n",
      "  Accuracy : 0.4421965317919075\n",
      "  F1 Score : 0.4655108294042282\n",
      "  Precision: 0.49459168422379485\n",
      "  Recall   : 0.4421965317919075\n",
      "\n",
      "MLP Classifier (trained on synthetic, tested on real 20%)\n",
      "  Accuracy : 0.5173410404624278\n",
      "  F1 Score : 0.48152628780757023\n",
      "  Precision: 0.4545107648487895\n",
      "  Recall   : 0.5173410404624278\n",
      "\n",
      "Deep Neural Network (Keras)\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "  Accuracy : 0.6791907514450867\n",
      "  F1 Score : 0.5494314168316536\n",
      "  Precision: 0.4613000768485415\n",
      "  Recall   : 0.6791907514450867\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "real_data = pd.read_csv(real_path)\n",
    "synthetic_data = pd.read_csv(fake_paths)\n",
    "\n",
    "# Encode categorical columns\n",
    "encoders = {}\n",
    "for col in real_data.columns:\n",
    "    if real_data[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        real_data[col] = le.fit_transform(real_data[col].astype(str))\n",
    "        synthetic_data[col] = le.transform(synthetic_data[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "# Split real data into train/test sets\n",
    "target_col = real_data.columns[-1]\n",
    "X_real = real_data.drop(columns=[target_col])\n",
    "y_real = real_data[target_col]\n",
    "\n",
    "X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(X_real, y_real, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare synthetic data\n",
    "X_syn = synthetic_data.drop(columns=[target_col])\n",
    "y_syn = synthetic_data[target_col]\n",
    "\n",
    "# Scale all features\n",
    "scaler = StandardScaler()\n",
    "X_real_train_scaled = scaler.fit_transform(X_real_train)\n",
    "X_real_test_scaled = scaler.transform(X_real_test)\n",
    "X_syn_scaled = scaler.transform(X_syn)\n",
    "\n",
    "# Train classifiers on synthetic data and test on real test set\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \"MLP Classifier\": MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_syn_scaled, y_syn)\n",
    "    preds = clf.predict(X_real_test_scaled)\n",
    "    print(f\"\\n{name} (trained on synthetic, tested on real 20%)\")\n",
    "    print(\"  Accuracy :\", accuracy_score(y_real_test, preds))\n",
    "    print(\"  F1 Score :\", f1_score(y_real_test, preds, average='weighted'))\n",
    "    print(\"  Precision:\", precision_score(y_real_test, preds, average='weighted'))\n",
    "    print(\"  Recall   :\", recall_score(y_real_test, preds, average='weighted'))\n",
    "\n",
    "# DNN with Keras (trained on synthetic, tested on real)\n",
    "print(\"\\nDeep Neural Network (Keras)\")\n",
    "num_classes = len(np.unique(y_syn))\n",
    "y_syn_cat = to_categorical(y_syn, num_classes=num_classes)\n",
    "y_real_test_cat = to_categorical(y_real_test, num_classes=num_classes)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_syn_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_syn_scaled, y_syn_cat, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "preds = model.predict(X_real_test_scaled)\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "print(\"  Accuracy :\", accuracy_score(y_real_test, pred_labels))\n",
    "print(\"  F1 Score :\", f1_score(y_real_test, pred_labels, average='weighted'))\n",
    "print(\"  Precision:\", precision_score(y_real_test, pred_labels, average='weighted'))\n",
    "print(\"  Recall   :\", recall_score(y_real_test, pred_labels, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL EVALUATION SUMMARY =====\n",
      "                 Model  Accuracy  F1 Score  Precision  Recall\n",
      "0  Logistic Regression    0.6936    0.6119     0.6087  0.6936\n",
      "1        Random Forest    0.7139    0.6844     0.6723  0.7139\n",
      "2              XGBoost    0.6040    0.5852     0.5784  0.6040\n",
      "3       MLP Classifier    0.7543    0.7284     0.7195  0.7543\n",
      "4          DNN (Keras)    0.7630    0.7162     0.7405  0.7630\n",
      "5              Average    0.7058    0.6652     0.6639  0.7058\n",
      "\n",
      "===== AVERAGE METRICS =====\n",
      "Average Accuracy: 0.7058\n",
      "Average F1 Score: 0.6652\n",
      "Average Precision: 0.6639\n",
      "Average Recall: 0.7058\n",
      "\n",
      "===== BEST MODEL =====\n",
      "Best model: MLP Classifier\n",
      "Accuracy: 0.7543\n",
      "F1 Score: 0.7284\n",
      "Precision: 0.7195\n",
      "Recall: 0.7543\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
